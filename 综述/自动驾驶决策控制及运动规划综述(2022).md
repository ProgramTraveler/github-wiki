# 自动驾驶决策控制及运动规划综述(2022)

[TOC]




## 背景

* 笔者的的研究方向是自动驾驶决策控制与运动规划(DMAP ---> Decision Making And Motion Planning)

## 介绍

<img src="D:/github-wiki/images/综述/自动驾驶决策控制及运动规划/001_1.png" alt="" style="zoom: 90%">

### 自动驾驶汽车存在的三大类感知类算法和决策控制方法

1.sequential planning(顺序规划)

* 或者叫流水线规划？ 属于传统方法 感知、决策、控制三个部分层次清晰
* 这也是我目前接触和正在学习的

2.behavior-aware planning(行为感知规划)

* 与上一个相比 ---> 引入了人机共驾、车路协同、车辆对外部动态环境的风险预估

3.end-to-end planning(端到端的规划)

* 基于 DL(深度学习)、DRL(深度强化学习) 
* 利用数据集做训练 ---> 从图像等感知信息到方向盘转向角等车辆控制输入的关系
* 时下热门方法之一
* 但是这部分的规划感觉还是在测试中 落地较难(难点在哪里？)

## Sequential Planning

* 路径规划 ---> 决策过程 ---> 车辆控制

### 无人车运动轨迹生成

1.直接轨迹生成法
2.路径-速度分解法
* 相比第一种 路径-速度难度更小 更加常用

### 路径规划可以分成四大类

[实现效果可以参考仓库](https://github.com/ProgramTraveler/PathPlanning)

#### 1.基于采样的算法

* PRM(概率路线图)、RRT(快速探索随机树)

##### PRM

* 预处理阶段 ---> 对状态空间内的安全区域均匀随机采样ｎ个点 每个采样点分别与一定距离内的邻近采样点连接 并丢弃掉与障碍物发生碰撞的轨迹 最终得到一个连通图
* 查询阶段 ---> 对于给定的一对初始和目标状态，分别将其连接到已经构建的图中 再使用搜索算法寻找满足要求的轨迹
* 容易看出 一旦构建一个 PRM 之后 可以用于解决不同初始、目标状态的运动规划问题 但是这个特性对于无人车运动规划而言是不必要的 另外 PRM 要求对状态之间作精确连接 这对于存在复杂微分约束的运动规划问题是十分困难的

##### RRT

* 树的初始化 ---> 初始化树的结点集和边集 结点集只包含初始状态 边集为空
* 树的生长 ---> 对状态空间随机采样 当采样点落在状态空间安全区域时 选择当前树中离采样点最近的结点 将其向采样点扩展(或连接) 若生成的轨迹不与障碍物发生碰撞 则将该轨迹加入树的边集 该轨迹的终点加入到树的结点集
* 重复步骤2 直至扩展到目标状态集中 相比 PRM 的无向图而言 RRT构建的是初始状态作为根结点、目标状态作为叶结点的树结构 对于不同的初始和目标状态 需要构建不同的树 另外RRT 不要求状态之间的精确连接 更适合解决像无人车运动规划这样的运动动力学问题

##### 面临的问题

* 几乎所有算法都有相同的问题 ---> 求解效率 是否是最优解
* RPM 和 RRT 拥有概率完备性的原因在于其几乎会遍历构型空间中的所有位置
* 优化 RRT 核心思想 ---> 引导树向空旷区域(远离障碍物 避免对障碍物处的节点进行重复检查) 具体方式如下
1)均匀采样
```word
(1)RRT-connect 算法 ---> 构建两棵树(初始状态和目标状态) 当两棵树长到一起时就找了解

(2)Go-biaing ---> 在随机采样序列中以一定比例插入目标状态 引导树向目标状态

(3)启发式 RRT ---> 启发式函数(增加扩展代价低的节点被采样的概率)

(4)f-biased 采样方法 ---> 先将状态空间离散化为网格 再使用 Dijkstra 算法计算每个网格上的代价 这个网格所在区域的点的代价值都等于该值 以此构建启发式函数
```
2)优化距离度量
```word
距离用来度量构形空间(状态空间)中两个构形(状态)之间路径(轨迹)的代价 这个代价可以理解为路径的长度、消耗的能量或是花费的时间 采用距离度量的原因在于辅助生成启发式代价函数 引导树的走向
```
3)降低碰撞检测次数
```word
碰撞检查是基于采样的算法的效率瓶颈之一 通常的做法是对路径等距离离散化 再对每个点处的构形作碰撞检查
```
4)提升实时性
5)在解决最优算法主要
```word
渐进最优性质的 PRM\*、RRG\*、RRT\* 算法
```

#### 2.基于搜索的算法

* A\*、D\*

#### 基于搜索的算法所面临的问题与建议

* A\* 本身属于静态规划的算法 针对 A\* 算法的延申有 weighted A\* 通过增加启发式函数的权重进一步引导搜索方向向这目标节点进行 搜索速度很快 但是容易陷入局部极小值 无法保证全局最优解
* 对于运动的车辆来说 使用 A\* 的衍生算法 D\*(dynamic A\*) 可大幅度提升效率 
* 动态规划为基础的还有 LPA\* 该算法可以处理状态格子的运动基元的代价是时变的情况当环境发生变化时可以通过对较少数目节点的重新搜索规划出新的最优路径 
* 在LPA\* 的基础上开发出 D*-Lite 可以获得与 D* 同样的结果 但是效率更高
* 在进行最优化解的探寻时 ARA\* 是在 Weighted A\* 基础上发展出的具有 Anytime 性质的搜索算法 它通过多次调用 Weighted A\* 算法且每次调用就缩小启发式函数的权重 这样算法可以快速求出可行解 通过引入集合 INCONS 使得每次循环可以继续使用上一次循环的信息 对路径做出优化 逐渐逼近最优解
* 在兼顾算法效率与最优性的问题上 Sandin aine 等提出了 MHA\* 算法 引入多个启发式函数 保证其中有一个启发式函数在单独使用时可以找到最优解 从而通过协调不同启发式函数生成的路径代价 可以兼顾算法的效率和最优性 DMHA\* 在 MHA\* 的基础上在线实时生成合适的启发式函数 从而避免局部最小值问题
* A* 还有其他的一些变种算法 M\* 是专门进行多机器人运动规划的算法 R\* 是由 A\* 结合随机采样而来的算法 可以一定程度上避免局部最优


#### 3.基于插值拟合的轨迹生成算法

* β样条曲线
* 基于插值拟合的算法可被定义为 ---> 根据已知的一系列用于描述道路图的点集 通过使用数据插值与曲线拟合的方式创造出智能车将行驶的路径 该路径可提供较好的连续性、较高的可导性 具体的方法如下
```word
1.Dubins 曲线和 Reeds and Sheep(RS) 曲线是连接构形空间中任意两点的最短路径 分别对应无倒车和有倒车的情况 它们都是由最大曲率圆弧和直线组成的 在圆弧和直线连接处存在曲率不连续 实际车辆按照这样曲线行驶时必须在曲率不连续处停车调整方向轮才能继续行驶
        
2.回旋线的曲率与曲线长度成正比关系 即路径的曲率与曲线长度成线性关系 它可以用作直线到圆弧之间的过渡曲线 从而改造 Dubins 曲线和 RS 曲线 实现曲率连续性 比较有代表性的是 CC-Steer 适用于低速下的运动规划
        
3.多项式插值曲线是最常用的一种方法 它可以通过满足结点的要求来设定多项式系数 并且获得较好的连续可导性 四阶多项式常用于纵向约束控制 五阶多项式常用于横向约束控制 三阶多项式也被用于超车轨迹中
```

* 样条曲线具有封闭的表达式 容易保证曲率连续性
* β 样条曲线可以实现曲率连续性 
* 三次Bezier 曲线可以保证曲率的连续性和有界性 并且计算量相对较小
* η^3 曲线是一种七次样条曲线 它有着很好的性质 ---> 曲率连续性和曲率导数的连续性 这对于高速行驶车辆是很有意义的

#### 4.局部路径规划的最又控制算法

* MPC(模型预测控制)
* 将基于最优控制的算法归在路径规划中 主要是因为其中的 MPC 可以进行局部的路径规划以进行避障 除此之外 MPC主要的作用是进行轨迹跟踪 其所考虑的问题除了必要的动力学、运动学约束以外 未来还应考虑舒适性 感知信息的不确定性 车间通信的不确定性 并且在局部轨迹规划时还可以将驾驶员纳入控制闭环
* MPC 所使用的预测模型有很多种 ---> 诸如卷积神经网络、模糊控制、状态空间等等 其中用的最多的为状态空间法 
* MPC可简要表述为 ---> 满足必要的动力学、运动学等等约束的情况下 通过数值手段(一般为数值手段 因为模型太过复杂 传统的变分法等解析方法不再适用)求解模型的最优解 该最优解即为状态方程的控制量 如方向盘转角等等 并将控制量作用在车模上以获得要求的状态量 如速度、加速度、坐标等等
* 通过上述描述可知 MPC的关键在于模型的建立与模型的求解 如何等效简化模型的建立以及提升求解的效率是重中之重 在不同的控制输入下车辆会走不同的轨迹 每一条轨迹都与之对应一个目标函数值 无人驾驶车辆会通过求解算法找出最小目标函数值对应的控制量 并将其作用在车上 如下图所示：
<img src="D:/github-wiki/images/综述/1-自动驾驶决策控制及运动规划/001_2.png" alt="" style="zoom: 90%">
* 为了降低建模难度 使用人工势能场模型进行建模 人工势能场的基本思想类似于电场 道路上的障碍物类似于电场中与场源相异电荷极性的电荷 具体效果如下图所示 障碍物(动态、静态)处的势能更高 无人车将向低势能位置前进
<img src="D:/github-wiki/images/综述/自动驾驶决策控制及运动规划/001_3.png" alt="" style="zoom: 70%">
<img src="D:/github-wiki/images/综述/自动驾驶决策控制及运动规划/001_4.png" alt="" style="zoom: 70%">
<img src="D:/github-wiki/images/综述/自动驾驶决策控制及运动规划/001_5.png" alt="" style="zoom: 70%">

## End To End DMAP

* 端到端的方法通常采用深度学习或者深度强化学习进行 
* end to end 方法输入到输出的映射有两种 一种是输入图像等感知信息输到出方向盘转角等控制量 另一个是感知信息到车模的状态量 如速度、坐标等 二者均需要大量的数据做支撑 通常在做网络训练时 会将图像进行随机平移旋转等操作 以此进行数据扩充
* 虚拟场景永远与实际场景有差距 因此通过虚拟场景训练的自动驾驶车辆需要做出针对这个差距的应对策略 为此需要估计神经网络(贝叶斯网络)的不确定性 然而 Recent ensemble、bootstrap、以及 Monte Carlo dropout methods 并不能准确的给出不确定性的大小
* 在训练强化学习网络时 不可能进行实车训练 一般都是进行仿真测试 通过仿真测试不仅可以获得大量的碰撞数据 还可以获得虚拟场景内的实际关系以便于训练 reward function

## Behavior-Aware Motion Planning

* Behavior-Aware Motion Planning 中文名为行为预警式运动规划 该方法相比两外两种特点在于将决策规划过程升级为交互式过程 包括驾驶员-驾驶车辆 驾驶车辆-外部环境 
* 研究这种方法的目的在于将外部环境的不确定性纳入决策规划当中 以此提高自动驾驶车辆行驶安全性 在研究过程中将不考虑 V2X 技术的辅助加持
* 本节将讲述研究中所用到的方法
```word
1.cooperation and interaction(协同与交流)
2.games-theoretic approaches(博弈论)
3.probalistic approach(概率方法)
4.partially observable markov decision process(隐马尔可夫)
5.learning based approaches(机器学习)
```

### Cooperation And Interaction

* 当自动驾驶车辆遇到突发状况 或是情况变得十分紧急 车辆会自动停止 这一行为被称为freezing-robot problem 如果此时车辆不停止而是继续前进的话 那么很有可能遇到碰撞的危险 

* 在处理车辆遇到上述问题时的不确定性问题时，通常选择以下三个方法：
```word
(1) 对外部环境进行更好的动态建模 将预测的外部环境的未来的状态也纳入到模型中 以此降低不确定性 但就算是完全知晓外部事物的运动状态 也仍然无法完全阻止 freezing-robot problem 的发生

(2) 建模时将外部事物对车辆的反应当作约束条件 然而该方法需要假定可以完全预测知晓外部事物 直观上来理解 过度的信任模型也将带来巨大的打击(那么信任还要不确定性干嘛)

(3) 将车辆与外部的事物看作为一体 使其具有相同的分布 如概率分布、价值函数分布
```

### Game-Theoretic Approaches

* 在控制算法中 通常情况下都是假定外部事物 如其他车辆 都是按照最小化其代价函数方法进行控制 评价其利益的手段则是一个 cost 函数或者 reward 函数或者 utility 函数 
* 除了上述控制方法外 另外一种则是构建在最大似然估计或是最大后验概率的角度上 来求某一函数的极值 同样都是求评价函数函数的最值 但是两种方法有所不同 
* 举例说明 ---> 第二种方法在遇到外来车辆时 先采取一个行动 随后对外来车辆进行建模 根据其行为来最大化自己的reward值 这种方法与在cooperation and interaction中提到的（2）有一个类似的缺点 就是建模的过程也就代表着对外部车辆间接的控制过程
* 在建立interaction型的model时 算法复杂度与agent的数量成指数型增长 因此提高求解效率就变得更加的重要 
* 最简单的方法就是根据agents运动情况 将行动空间离散化 然后搜索整个空间以获得可选的行动

### Probalilistic Approaches

* 这个方法就是跟字面表达的一样了 其实其他几种方法或多或少都是基于概率来构建算法的

### Partially Observable Markov Decision Processes

* 以上简称POMDP 它是probilistic方法的一个分支 隐马尔可夫决策过程可以将其它agents的意图纳为自己的隐变量
* POMDP社区关注offline建模 offline意味着想要获得最优的模 ，但这使得算法运行起来耗时巨大的 对于自动驾驶汽车来说是无法接受的 为了提升效率 可使POMDP仅预测目前可到达的状态 进行规划轨迹时只规划个大概 而不是规划详细轨迹

### Learning-Based Approaches

<img src="D:/github-wiki/images/综述/自动驾驶决策控制及运动规划/001_6.png" alt="" style="zoom: 90%">

* 如上图所示为学习类算法最原始的分类 前面提到过的深度学习属于机器学习中的神经网络加深版(除此之外没啥区别) 深度强化学习是将神经网络中的个别函数用强化学习的方式来学 无监督学习主要用在感知部分 决策控制部分的应用更多的是另外三类 以及他们的结合
* 逆向强化学习IRL也被称为逆向最优控制，它可以为强化学习提供通常难以表述的reward函数 并且还不易发生过拟合 RL在此基础上在寻找最优的policy

## 